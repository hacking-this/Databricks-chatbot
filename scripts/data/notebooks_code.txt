Develop code in Databricks notebooks | Databricks Documentation


Skip to main content
Get started
Developers
Reference
Release notes
Resources
Support
Knowledge Base
Community
Training
Feedback
English
English
日本語
Português
AWS
Azure
GCP
SAP
Try Databricks
Databricks on AWS
Sign up for Databricks
Get started tutorials
What is Databricks?
Architecture
Workspace UI
Data guides
Data engineering
AI and machine learning
OLTP databases
Data warehousing
Business intelligence
Compute
Notebooks
Basic editing
Develop code in notebooks
Code navigation
Code help using Databricks Assistant
Notebook outputs and results
Share code between notebooks
Debug notebooks
Version history
Databricks widgets
Package cells
IPython kernel
Unit testing
Run notebooks
Collaborate using notebooks
Manage notebooks
Customize notebook appearance
Best practices
Limitations
Tables
Apache Spark
Developers
Technology partners
Administration
Security and compliance
Data governance (Unity Catalog)
Reference
Release notes
Resources
Notebooks
Develop code in notebooks
On this page
Develop code in Databricks notebooks


This page describes how to develop code in Databricks notebooks, including autocomplete, automatic formatting for Python and SQL, combining Python and SQL in a notebook, and tracking the notebook version history.


For more details about advanced functionality available with the editor, such as autocomplete, variable selection, multi-cursor support, and side-by-side diffs, see 
Navigate the Databricks notebook and file editor
.


When you use the notebook or the file editor, Databricks Assistant is available to help you generate, explain, and debug code. See 
Use Databricks Assistant 
 for more information.


Databricks notebooks also include a built-in interactive debugger for Python notebooks. See 
Debug notebooks
.


Modularize your code
​


With 
Databricks Runtime
 11.3 LTS and above, you can create and manage source code files in the 
Databricks
 workspace, and then import these files into your notebooks as needed.


For more information on working with source code files, see 
Share code between Databricks notebooks
 and 
Work with Python and R modules
.


Format code cells
​


Databricks
 provides tools that allow you to format Python and SQL code in notebook cells quickly and easily. These tools reduce the effort to keep your code formatted and help to enforce the same coding standards across your notebooks.


Python black formatter library
​


Preview
This feature is in 
Public Preview
.


Databricks
 supports Python code formatting using 
black
 within the notebook. The notebook must be attached to a cluster with 
black
 and 
tokenize-rt
 Python packages installed.


On 
Databricks Runtime
 11.3 LTS and above, 
Databricks
 preinstalls 
black
 and 
tokenize-rt
. You can use the formatter directly without needing to install these libraries.


On 
Databricks Runtime
 10.4 LTS and below, you must install 
black==22.3.0
 and 
tokenize-rt==4.2.1
 from PyPI on your notebook or cluster to use the Python formatter. You can run the following command in your notebook:


%pip install black==22.3.0 tokenize-rt==4.2.1


or 
install the library on your cluster
.


For more details about installing libraries, see 
Python environment management
.


For files and notebooks in Databricks Git folders, you can configure the Python formatter based on the 
pyproject.toml
 file. To use this feature, create a 
pyproject.toml
 file in the Git folder root directory and configure it according to the 
Black configuration format
. Edit the [tool.black] section in the file. The configuration is applied when you format any file and notebook in that Git folder.


How to format Python and SQL cells
​


You must have 
CAN EDIT permission
 on the notebook to format code.


Databricks
 uses the 
Gethue/sql-formatter
 library to format SQL and the 
black
 code formatter for Python.


You can trigger the formatter in the following ways:






Format a single cell




Keyboard shortcut: Press 
Cmd+Shift+F
.


Command context menu:




Format SQL cell: Select 
Format SQL
 in the command context dropdown menu of a SQL cell. This menu item is visible only in SQL notebook cells or those with a 
%sql
 
language magic
.


Format Python cell: Select 
Format Python
 in the command context dropdown menu of a Python cell. This menu item is visible only in Python notebook cells or those with a 
%python
 
language magic
.






Notebook 
Edit
 menu: Select a Python or SQL cell, and then select 
Edit > Format Cell(s)
.








Format multiple cells


Select multiple cells
 and then select 
Edit > Format Cell(s)
. If you select cells of more than one language, only SQL and Python cells are formatted. This includes those that use 
%sql
 and 
%python
.






Format all Python and SQL cells in the notebook


Select 
Edit > Format Notebook
. If your notebook contains more than one language, only SQL and Python cells are formatted. This includes those that use 
%sql
 and 
%python
.






To customize how your SQL queries are formatted, see 
Custom format SQL statements
.


Limitations of code formatting
​




Black enforces 
PEP 8
 standards for 4-space indentation. Indentation is not configurable.


Formatting embedded Python strings inside a SQL UDF is not supported. Similarly, formatting SQL strings inside a Python UDF is not supported.




Code languages in notebooks
​


Set default language
​


The default language for the notebook appears below the notebook name.




To change the default language, click the language button and select the new language from the dropdown menu. To ensure that existing commands continue to work, commands of the previous default language are automatically prefixed with a language magic command.


Mix languages
​


By default, cells use the default language of the notebook. You can override the default language in a cell by clicking the language button and selecting a language from the dropdown menu.




Alternately, you can use the language magic command 
%<language>
 at the beginning of a cell. The supported magic commands are: 
%python
, 
%r
, 
%scala
, and 
%sql
.


note
When you invoke a language magic command, the command is dispatched to the REPL in the execution context for the notebook. Variables defined in one language (and hence in the REPL for that language) are not available in the REPL of another language. REPLs can share state only through external resources such as files in DBFS or objects in object storage.


Notebooks also support a few auxiliary magic commands:




%sh
: Allows you to run shell code in your notebook. To fail the cell if the shell command has a non-zero exit status, add the 
-e
 option. This command runs only on the 
Apache Spark
 driver, and not the workers. To run a shell command on all nodes, use an 
init script
.


%fs
: Allows you to use 
dbutils
 filesystem commands. For example, to run the 
dbutils.fs.ls
 command to list files, you can specify 
%fs ls
 instead. For more information, see 
Work with files on 
Databricks
.


%md
: Allows you to include various types of documentation, including text, images, and mathematical formulas and equations. See the next section.




SQL syntax highlighting and autocomplete in Python commands
​


Syntax highlighting and SQL 
autocomplete
 are available when you use SQL inside a Python command, such as in a 
spark.sql
 command.


Explore SQL cell results
​


In a Databricks notebook, results from a SQL language cell are automatically made available as an implicit DataFrame assigned to the variable 
_sqldf
. You can then use this variable in any Python and SQL cells you run afterward, regardless of their position in the notebook.


note
This feature has the following limitations:


The 
_sqldf
 variable is not available in notebooks that use a 
SQL warehouse
 for compute.


Using 
_sqldf
 in subsequent Python cells is supported in 
Databricks Runtime
 13.3 and above.


Using 
_sqldf
 in subsequent SQL cells is only supported on 
Databricks Runtime
 14.3 and above.


If the query uses the keywords 
CACHE TABLE
 or 
UNCACHE TABLE
, the 
_sqldf
 variable is not available.




The screenshot below shows how 
_sqldf
 can be used in subsequent Python and SQL cells:




important
The variable 
_sqldf
 is reassigned each time a SQL cell is run. To avoid losing reference to a specific DataFrame result, assign it to a new variable name before you run the next SQL cell:
Python
SQL
Python
new_dataframe_name 
=
 _sqldf
SQL
ALTER
 
VIEW
 _sqldf 
RENAME
 
TO
 new_dataframe_name


Execute SQL cells in parallel
​


While a command is running and your notebook is attached to an interactive cluster, you can run a SQL cell simultaneously with the current command. The SQL cell is executed in a new, parallel session.


To execute a cell in parallel:






Run the cell
.






Click 
Run now
. The cell is immediately executed.








Because the cell is run in a new session, temporary views, UDFs, and the 
implicit Python DataFrame
 (
_sqldf
) are not supported for cells that are executed in parallel. In addition, the default catalog and database names are used during parallel execution. If your code refers to a table in a different catalog or database, you must specify the table name using three-level namespace (
catalog
.
schema
.
table
).


Execute SQL cells on a SQL warehouse
​


You can run SQL commands in a Databricks notebook on a 
SQL warehouse
, a type of compute that is optimized for SQL analytics. See 
Use a notebook with a SQL warehouse
.


Run code in Assistant on serverless compute
​


You can run code in the Assistant on Databricks on serverless compute, which is the default compute on Databricks. For pages where there is already a compute selected (for example, notebooks or SQL editor), Databricks defaults to using it. For information about compute types, see 
Types of compute
.


If you don't have access to serverless compute, you must have a compute instance available to run code in the Assistant panel.
Last updated
 on 
Jun 25, 2025
Modularize your code
Format code cells
Python black formatter library
How to format Python and SQL cells
Limitations of code formatting
Code languages in notebooks
Set default language
Mix languages
SQL syntax highlighting and autocomplete in Python commands
Explore SQL cell results
Execute SQL cells in parallel
Execute SQL cells on a SQL warehouse
Run code in Assistant on serverless compute
Was this article helpful?
Send us feedback
·
Privacy Notice
·
Terms of Use
·
Modern Slavery Statement
·
California Privacy
·
Your Privacy Choices 
© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the 
Apache Software Foundation
.






 




We Care About Your Privacy
Databricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our 
Cookie Notice
. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”

Manage Preferences
Reject All
 
Accept All
Privacy Preference Center
Privacy Preference Center
Your Privacy
Strictly Necessary Cookies
Performance Cookies
Functional Cookies
Targeting Cookies
TOTHR
Your Privacy
When you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.

            
More information
Strictly Necessary Cookies
Always Active
These cookies are necessary for the website to function and cannot be switched off in our systems. They assist with essential site functionality such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will no longer work.
Performance Cookies
 
 
Performance Cookies
 
These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.
Functional Cookies
 
 
Functional Cookies
 
These cookies enable the website to provide enhanced functionality and personalization. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.
Targeting Cookies
 
 
Targeting Cookies
 
These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant advertisements on other sites. If you do not allow these cookies, you will experience less targeted advertising.
TOTHR
 
 
TOTHR
 
 
Back Button
Cookie List
 
Filter Button
Consent
 
Leg.Interest
 
checkbox label
 
label
 
checkbox label
 
label
 
checkbox label
 
label
Clear
 
checkbox label
 
label
Apply
 
Cancel
Confirm My Choices
 
Allow All

--- CODE SNIPPETS ---

%pip install black==22.3.0 tokenize-rt==4.2.1

%pip install black==22.3.0 tokenize-rt==4.2.1

new_dataframe_name = _sqldf

new_dataframe_name = _sqldf

ALTER VIEW _sqldf RENAME TO new_dataframe_name

ALTER VIEW _sqldf RENAME TO new_dataframe_name