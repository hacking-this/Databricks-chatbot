Compute | Databricks Documentation


Skip to main content
Get started
Developers
Reference
Release notes
Resources
Support
Knowledge Base
Community
Training
Feedback
English
English
日本語
Português
AWS
Azure
GCP
SAP
Try Databricks
Databricks on AWS
Sign up for Databricks
Get started tutorials
What is Databricks?
Architecture
Workspace UI
Data guides
Data engineering
AI and machine learning
OLTP databases
Data warehousing
Business intelligence
Compute
Serverless compute
All-purpose and jobs compute
SQL warehouses
Pools
What is Photon?
Notebooks
Tables
Apache Spark
Developers
Technology partners
Administration
Security and compliance
Data governance (Unity Catalog)
Reference
Release notes
Resources
Compute
On this page
Compute


Databricks
 compute refers to the selection of computing resources available in the 
Databricks
 workspace. Users need access to compute to run data engineering, data science, and data analytics workloads, such as production ETL pipelines, streaming analytics, ad-hoc analytics, and machine learning.


Users can either connect to existing compute or create new compute if they have the proper permissions.


You can view the compute you have access to using the 
Compute
 section of the workspace:




Types of compute
​


These are the types of compute available in 
Databricks
:




Serverless compute for notebooks
: On-demand, scalable compute used to execute SQL and Python code in notebooks.


Serverless compute for jobs
: On-demand, scalable compute used to run your Lakeflow Jobs without configuring and deploying infrastructure.


All-purpose compute
: Provisioned compute used to analyze data in notebooks. You can create, terminate, and restart this compute using the UI, CLI, or REST API.


Jobs compute
: Provisioned compute used to run automated jobs. The 
Databricks
 job scheduler automatically creates a job compute whenever a job is configured to run on new compute. The compute terminates when the job is complete. You 
cannot
 restart a job compute. See 
Configure compute for jobs
.


Instance pools
: Compute with idle, ready-to-use instances, used to reduce start and autoscaling times. You can create this compute using the UI, CLI, or REST API.






Serverless SQL warehouses
: On-demand elastic compute used to run SQL commands on data objects in the SQL editor or interactive notebooks. You can create SQL warehouses using the UI, CLI, or REST API.






Classic SQL warehouses
: Used to run SQL commands on data objects in the SQL editor or interactive notebooks. You can create SQL warehouses using the UI, CLI, or REST API.




The articles in this section describe how to work with compute resources using the 
Databricks
 UI. For other methods, see 
What is the Databricks CLI?
 and the 
Databricks REST API reference
.


Databricks Runtime
​


Databricks Runtime
 is the set of core components that run on your compute. The 
Databricks Runtime
 is a configurable setting in all-purpose of jobs compute but autoselected in SQL warehouses.


Each 
Databricks Runtime
 version includes updates that improve the usability, performance, and security of big data analytics. The 
Databricks Runtime
 on your compute adds many features, including:




Delta Lake, a next-generation storage layer built on top of Apache Spark that provides ACID transactions, optimized layouts and indexes, and execution engine improvements for building data pipelines. See 
What is 
Delta Lake
 in 
Databricks
?
.


Installed Java, Scala, Python, and R libraries.


Ubuntu and its accompanying system libraries.


GPU libraries for GPU-enabled clusters.


Databricks
 services that integrate with other components of the platform, such as notebooks, jobs, and cluster management.




For information about the contents of each runtime version, see the 
release notes
.


Runtime versioning
​


Databricks Runtime
 versions are released on a regular basis:




Long Term Support
 versions are represented by an 
LTS
 qualifier (for example, 
3.5 LTS
). For each major release, we declare a “canonical” feature version, for which we provide three full years of support. See 
Databricks support lifecycles
 for more information.


Major
 versions are represented by an increment to the version number that precedes the decimal point (the jump from 3.5 to 4.0, for example). They are released when there are major changes, some of which may not be backwards-compatible.


Feature
 versions are represented by an increment to the version number that follows the decimal point (the jump from 3.4 to 3.5, for example). Each major release includes multiple feature releases. Feature releases are always backward compatible with previous releases within their major release.


Last updated
 on 
Jan 22, 2025
Types of compute
Databricks Runtime
Runtime versioning
Was this article helpful?
Send us feedback
·
Privacy Notice
·
Terms of Use
·
Modern Slavery Statement
·
California Privacy
·
Your Privacy Choices 
© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the 
Apache Software Foundation
.






 




We Care About Your Privacy
Databricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our 
Cookie Notice
. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”

Manage Preferences
Reject All
 
Accept All
Privacy Preference Center
Privacy Preference Center
Your Privacy
Strictly Necessary Cookies
Performance Cookies
Functional Cookies
Targeting Cookies
TOTHR
Your Privacy
When you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.

            
More information
Strictly Necessary Cookies
Always Active
These cookies are necessary for the website to function and cannot be switched off in our systems. They assist with essential site functionality such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will no longer work.
Performance Cookies
 
 
Performance Cookies
 
These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.
Functional Cookies
 
 
Functional Cookies
 
These cookies enable the website to provide enhanced functionality and personalization. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.
Targeting Cookies
 
 
Targeting Cookies
 
These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant advertisements on other sites. If you do not allow these cookies, you will experience less targeted advertising.
TOTHR
 
 
TOTHR
 
 
Back Button
Cookie List
 
Filter Button
Consent
 
Leg.Interest
 
checkbox label
 
label
 
checkbox label
 
label
 
checkbox label
 
label
Clear
 
checkbox label
 
label
Apply
 
Cancel
Confirm My Choices
 
Allow All

--- CODE SNIPPETS ---

