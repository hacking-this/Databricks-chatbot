# Reading and Writing Data using PySpark

In Databricks, you can use PySpark for reading/writing Delta and other formats.

## Reading from Delta:
```python
df = spark.read.format("delta").load("/mnt/delta/customers")
```

## Writing to Delta:
```python
df.write.format("delta").mode("overwrite").save("/mnt/delta/output")
```

## Other formats:
```python
df = spark.read.option("header", "true").csv("/mnt/data/sales.csv")
df.write.parquet("/mnt/data/output")
```

## Notes:
- `.mode("overwrite")` replaces existing data.
- Always prefer Delta for reliability and transaction support.
